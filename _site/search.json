[
  {
    "objectID": "Optimisation_Prac.html",
    "href": "Optimisation_Prac.html",
    "title": "Optimisation Practical",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "Optimisation_Prac.html#question-1-bisection-method",
    "href": "Optimisation_Prac.html#question-1-bisection-method",
    "title": "Optimisation Practical",
    "section": "Question 1: Bisection Method",
    "text": "Question 1: Bisection Method\nUse the bisection method to find the maximum of \\(g(x)\\):\n\\[\n\\begin{aligned}\ng(x) &= \\frac{log(x)}{1+x} \\\\\n\\implies g'(x) &= \\frac{-log(x) + \\frac{1}{x}(1+x)}{1+x} \\\\\n\\implies g'(x) &= \\frac{-x \\ log(x) + (1+x)}{x+x^2}\n\\end{aligned}\n\\]\n\n## Question 1\ng_dash_func &lt;- function(x)\n{\n  return((1 + x - x*log(x))/(x + x^2)^2)\n}\n\nbisection_method &lt;- function(derivative, a, b, precision = 1e-12, iter = 1000)\n{\n  for (i in 1:iter)\n  {\n    x &lt;- (a + b)/2\n    grad_at_x &lt;- derivative(x)\n    decision &lt;- derivative(a)*grad_at_x\n    if (abs(a - x) &lt; precision)\n      return(x)\n    else if (decision &lt;= 0)\n      b &lt;- x\n    else\n      a &lt;- x\n  }\n  \n  # If precision not reached, returns NA\n  return(NA)\n}\n\n## Question 2\nx &lt;- bisection_method(g_dash_func, 0.1, 10)\n\n## Question 3\nuniroot(g_dash_func, c(0.1,10))\n\n$root\n[1] 3.591118\n\n$f.root\n[1] 1.690684e-08\n\n$iter\n[1] 9\n\n$init.it\n[1] NA\n\n$estim.prec\n[1] 6.103516e-05"
  },
  {
    "objectID": "Optimisation_Prac.html#question-2-maximum-likelihood",
    "href": "Optimisation_Prac.html#question-2-maximum-likelihood",
    "title": "Optimisation Practical",
    "section": "Question 2: Maximum Likelihood",
    "text": "Question 2: Maximum Likelihood\nAssume that these 13 observations are from a Poisson distribution, with rate parameter \\(\\lambda\\):\n\ncounts &lt;- c(3, 1, 1, 3, 1, 4, 3, 2, 0, 5, 0, 4, 2)\n\n\n## Question 1\npoi_likelihood &lt;- function(lambda, obs)\n{\n  product &lt;- 1\n  \n  for (i in obs) \n  {\n    product = product * (((lambda^i)*exp(-lambda))/(factorial(i)))\n  }\n  \n  return(product)\n}\n\npoi_log_likelihood &lt;- function(lambda, obs)\n{\n  sum &lt;- 0\n  \n  for (i in obs) \n  {\n    sum = sum + i*log(lambda) - lambda - log(factorial(i))\n  }\n  \n  return(sum)\n}\n\n## Question 2\nlambdas &lt;- seq(0, 10, length = 1000)\nvectorised_poi_likelihood &lt;- Vectorize(poi_likelihood, \"lambda\")\n\ndata &lt;- data.frame(x = lambdas, y = vectorised_poi_likelihood(lambdas, counts))\nggplot(data = data, aes(x, y)) +\n  geom_line()\n\n\n\n## Question 3\n# ?optim\n# ?nlm\n\n# Question 4\nneg_poi_likelihood &lt;- function(lambda, obs)\n{\n  return(-1*poi_likelihood(lambda, obs))\n}\nneg_poi_log_likelihood &lt;- function(lambda, obs)\n{\n  return(-1*poi_log_likelihood(lambda, obs))\n}\nMLE &lt;- optim(2, neg_poi_likelihood, obs = counts)      # Note this doesn't work!!\nMLE &lt;- optim(2, neg_poi_log_likelihood, obs = counts)\n\nMLE &lt;- nlm(neg_poi_likelihood, p = 2, obs = counts)    # Note this doesn't work!!\nMLE &lt;- nlm(neg_poi_log_likelihood, p = 2, obs = counts)\n\n## Question 5\npoi_log_likelihood_d1 &lt;- function(lambda, obs)\n{\n  sum &lt;- 0\n  for (i in obs) \n  {\n    sum = sum + i/lambda - 1\n  }\n  \n  return(sum)\n}\npoi_log_likelihood_d2 &lt;- function(lambda, obs)\n{\n  sum &lt;- 0\n  for (i in obs) \n  {\n    sum = sum - i/(lambda)^2\n  }\n  return(sum)\n}\n\nnewtons_method &lt;- function(d1, d2, guess, obs, precision = 1e-12, iter = 1000)\n{\n  for (i in 1:iter)\n  {\n    d1_value  &lt;- d1(guess, obs)\n    d2_value  &lt;- d2(guess, obs)\n    new_guess &lt;- guess - d1_value/d2_value\n    if (abs(new_guess - guess) &lt; precision)\n      return(new_guess)\n    else\n      guess = new_guess\n  }\n  \n  # If precision not obtained\n  return(NA)\n}\n\nMLE &lt;- newtons_method(poi_log_likelihood_d1, poi_log_likelihood_d2, 2, counts)\n\nggplot(data = data, aes(x, y)) +\n  geom_line() +\n  geom_vline(xintercept = MLE, color = \"red\") +\n  annotate(\"text\", x = MLE, y = max(data$y), \n           label = paste(\"MLE =\", round(MLE, 5)), color = \"red\", hjust = -0.1)"
  },
  {
    "objectID": "Optimisation_Prac.html#question-3-maximum-likelihood-using-nlm-and-gauss-seidel",
    "href": "Optimisation_Prac.html#question-3-maximum-likelihood-using-nlm-and-gauss-seidel",
    "title": "Optimisation Practical",
    "section": "Question 3: Maximum Likelihood using nlm and Gauss-Seidel",
    "text": "Question 3: Maximum Likelihood using nlm and Gauss-Seidel\nFor the following data assume the model (Poisson regression):\n\\[\nY_i \\sim Poisson(\\lambda_i) \\qquad \\qquad \\lambda_i  = \\alpha + \\beta  x_i\n\\]\n\ny &lt;- c(2,4,3,0,1,4,3,6,10,7)\n\nx &lt;- c(0.49909145, 1.24373850, 0.34376255, 0.03833630, 0.09699331, \n       0.19469526, 0.21237902, 1.56276200, 1.56909233, 1.88487024)\n\n\n## Question 1\npoi_log_likelihood &lt;- function(pars, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- pars[1] + pars[2] * x_obs[i]\n    sum &lt;- sum + y_obs[i]*log(lambda_i) - lambda_i - log(factorial(y_obs[i]))\n  }\n  return(sum)\n}\n\nneg_poi_log_likelihood &lt;- function(pars, y_obs, x_obs)\n{\n  return(-1*poi_log_likelihood(pars, y_obs, x_obs))\n}\n\nnlm(neg_poi_log_likelihood, c(1,1), y_obs = y, x_obs = x)$estimate\n\n[1] 1.197778 3.665084\n\n## Question 2\nd1_log_likelihood_alpha &lt;- function(alpha, beta, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- alpha + beta * x_obs[i]\n    sum &lt;- sum + y_obs[i]/lambda_i - 1\n  }\n  return(sum)\n}\n\nd1_log_likelihood_beta &lt;- function(alpha, beta, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- alpha + beta * x_obs[i]\n    sum &lt;- sum + (y_obs[i]/lambda_i - 1) * x_obs[i]\n  }\n  return(sum)\n}\n\ngauss_seidel &lt;- function(pars, y_obs, x_obs, precision = 1e-15)\n{\n  curr_pars &lt;- c(Inf, Inf)\n  while (!all(abs(curr_pars - pars) &lt; precision))\n  {\n    curr_pars &lt;- pars\n    pars[1] &lt;- uniroot(d1_log_likelihood_alpha, c(0,10), beta = pars[2], \n                   y_obs = y_obs, x_obs = x_obs)$root\n    pars[2] &lt;- uniroot(d1_log_likelihood_beta, c(0,10), alpha = pars[1], \n                   y_obs = y_obs, x_obs = x_obs)$root\n  }\n  return(curr_pars)\n}\n\ngauss_seidel(c(1,1), y_obs = y, x_obs = x)\n\n[1] 1.197788 3.665062\n\n## Question 3\n# Yes\n\n## Question 4\nd2_log_likelihood_alpha &lt;- function(pars, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- pars[1] + pars[2] * x_obs[i]\n    sum &lt;- sum - y_obs[i]/(lambda_i^2)\n  }\n  return(sum)\n}\n\nd2_log_likelihood_beta &lt;- function(pars, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- pars[1] + pars[2] * x_obs[i]\n    sum &lt;- sum - (y_obs[i]/(lambda_i^2)) * (x_obs[i]^2)\n  }\n  return(sum)\n}\n\nd2_log_likelihood_alpha_beta &lt;- function(pars, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- pars[1] + pars[2] * x_obs[i]\n    sum &lt;- sum - (y_obs[i]/(lambda_i^2)) * x_obs[i]\n  }\n  return(sum)\n}\n\nd2_log_likelihood_beta_alpha &lt;- function(pars, y_obs, x_obs)\n{\n  sum &lt;- 0\n  for (i in 1:length(y_obs)) \n  {\n    lambda_i &lt;- pars[1] + pars[2] * x_obs[i]\n    sum &lt;- sum - (y_obs[i]/(lambda_i^2)) * x_obs[i]\n  }\n  return(sum)\n}\n\nnewtons_method &lt;- function(pars, y_obs, x_obs, precision = 1e-18)\n{\n  curr_pars &lt;- c(Inf, Inf)\n  while (!all(abs(curr_pars - pars) &lt; precision))\n  {\n    curr_pars &lt;- pars\n    pars &lt;- pars - matrix(c(d1_log_likelihood_alpha(curr_pars[1], curr_pars[2], y_obs, x_obs), \n                            d1_log_likelihood_beta(curr_pars[1], curr_pars[2], y_obs, x_obs)),\n                          byrow = T, ncol = 2) %*% \n      solve(matrix(c(d2_log_likelihood_alpha(curr_pars, y_obs, x_obs), \n                     d2_log_likelihood_alpha_beta(curr_pars, y_obs, x_obs),\n                     d2_log_likelihood_beta_alpha(curr_pars, y_obs, x_obs), \n                     d2_log_likelihood_beta(curr_pars, y_obs, x_obs)), \n                   byrow = T, nrow = 2))\n  }\n  return(as.vector(curr_pars))\n}\n\nnewtons_method(c(1,1), y_obs = y, x_obs = x)\n\n[1] 1.197777 3.665086\n\n## Question 5\n# You would use Gauss-Seidel when the 2nd derivative cannot be calculated\n# If it can be calculated, Newtons method converges much quicker.\n\n## Question 6\n# Confirming answers\nas.vector(glm(y~x, family = poisson(link = \"identity\"))$coefficients)\n\n[1] 1.197764 3.665103"
  },
  {
    "objectID": "Optimisation_Prac.html#question-4-poisson-regression-with-newtons-method",
    "href": "Optimisation_Prac.html#question-4-poisson-regression-with-newtons-method",
    "title": "Optimisation Practical",
    "section": "Question 4: Poisson Regression with Newton’s Method",
    "text": "Question 4: Poisson Regression with Newton’s Method\nThe following data are from a Poisson distribution with \\(\\mu_i = \\alpha + \\beta x_i\\), i.e., every observation has a different mean (expected value), the mean depends on an explanatory variable \\(x_i\\). Note: no log-link.\nThere are only two parameters. From the code below you should learn how to:\n\nfind derivatives in R,\ncreate and understand contour plots,\nwrite an algorithm for Newton’s method for more than one-dimensional problems.\nuse expressions and evaluate them\n\n\ny &lt;- c(2, 4, 3, 0, 1, 4, 3, 6, 10, 7)\n\nx &lt;- c(0.49909145, 1.24373850, 0.34376255, 0.03833630, 0.09699331, 0.19469526,\n       0.21237902, 1.56276200, 1.56909233, 1.88487024)\n\nllik.full &lt;- function(p) {                         # p is a vector of 2 parameters\n  ll &lt;- sum(dpois(y, lambda = p[1] + p[2] * x, log = TRUE))\n  return(-ll)               \n}\n\n# parameter values over which want to calculate loglik\np1.v &lt;- seq(0,3.5,length=100)                \np2.v &lt;- seq(0.1,8,length=100)\n\n# create an array with dimensions 10000 and 2, with \n# every combination of the values \npp &lt;- expand.grid(x = p1.v, y = p2.v)      \n                                           \nz &lt;- numeric(length(pp$x))\n\n# calculate loglik at every combination of param. values\nfor(i in 1:length(pp$x)) {\n  z[i] &lt;- llik.full(c(pp$x[i],pp$y[i]))          \n}\n\nZ &lt;- matrix(z, nrow = 100)                 \n\ncontour(p1.v, p2.v, Z, add = F, nlevels = 20)\n\n\n\nf &lt;- expression(-(alpha + beta * x) + y * log(alpha + beta * x))\n\nda &lt;- D(f, \"alpha\")\ndb &lt;- D(f, \"beta\")\n(dab &lt;- D(da, \"beta\"))\n\n-(y * (x/(alpha + beta * x)^2))\n\n(daa &lt;- D(da, \"alpha\"))\n\n-(y * (1/(alpha + beta * x)^2))\n\n(dbb &lt;- D(db, \"beta\"))  \n\n-(y * (x * x/(alpha + beta * x)^2))\n\ninit &lt;- c(3, 1)\ntol &lt;- 0.01\nl.current &lt;- init\nalpha &lt;- l.current[1]\nbeta &lt;- l.current[2]\n\ncontour(p1.v, p2.v, Z, add = FALSE, nlevels = 20, \n        xlab = expression(alpha), ylab = expression(beta), \n        main = \"log-likelihood surface\", las = 1)\npoints(alpha, beta, col = \"purple\", pch = 20)\n\ng1 &lt;- c(sum(eval(da)), sum(eval(db)))\nH &lt;- matrix(c(sum(eval(daa)), sum(eval(dab)), \n              sum(eval(dab)), sum(eval(dbb))), nrow = 2)\n\nl.new &lt;- l.current - g1 %*% solve(H)\nalpha &lt;- l.new[1]\nbeta &lt;- l.new[2]\n\nlines(c(alpha, l.current[1]), c(beta, l.current[2]), \n      col = \"purple\", pch = 20, type = \"b\")\n\nfor (i in 1:9) {\n  l.current &lt;- l.new\n  \n  g1 &lt;- c(sum(eval(da)), sum(eval(db)))\n  H &lt;- matrix(c(sum(eval(daa)), sum(eval(dab)), \n                sum(eval(dab)), sum(eval(dbb))), nrow = 2)\n  \n  l.new &lt;- l.current - g1 %*% solve(H)\n  alpha &lt;- l.new[1]\n  beta &lt;- l.new[2]\n  print(c(l.new))\n  \n  lines(c(alpha, l.current[1]), c(beta, l.current[2]), \n        col = \"purple\", pch = 20, type = \"b\")\n}\n\n[1] 0.9534927 3.7604267\n[1] 1.167730 3.684351\n[1] 1.197372 3.665358\n[1] 1.197777 3.665087\n[1] 1.197777 3.665086\n[1] 1.197777 3.665086\n[1] 1.197777 3.665086\n[1] 1.197777 3.665086\n[1] 1.197777 3.665086\n\nout &lt;- optim(par = init, fn = llik.full)\npoints(out$par[1], out$par[2], pch = 19, col = \"red\")"
  },
  {
    "objectID": "Optimisation_Prac.html#question-5-constrained-linear-optimization",
    "href": "Optimisation_Prac.html#question-5-constrained-linear-optimization",
    "title": "Optimisation Practical",
    "section": "Question 5: Constrained (linear) Optimization",
    "text": "Question 5: Constrained (linear) Optimization\nMaximize \\(2x + 2y + 3z\\) subject to:\n\\[\n\\begin{align*}\n-2x + y + z & \\le 1 \\\\\n4x - y + 3z & \\le 3 \\\\\nx \\ge 0, y \\ge 0, z & \\ge 0\\\\\n\\end{align*}\n\\]\nconstrOptim(theta, f, grad, ui, ci): Look at its help function and the example at the bottom of the help page.\nFor constrOptim the constraints need to be rephrased such that \\(ui \\cdot \\theta - ci &gt;= 0\\), where \\(\\theta\\) is the parameter vector.\n\\[\n\\begin{align*}\n2x - y - z + 1 & \\ge 0 \\\\\n-4x + y - 3z + 3 & \\ge 0 \\\\\nx \\ge 0, y \\ge 0, z & \\ge 0\\\\\n\\end{align*}\n\\]\n\ntheta_init &lt;- c(0.1, 1, 0.1)\nminimise &lt;- function(xyz) c(crossprod(c(-2,-2,-3), xyz))\n\nui_matrix &lt;- matrix(c( 2, -1, -1,\n                      -4,  1, -3,\n                       1,  0,  0,\n                       0,  1,  0,\n                       0,  0,  1),\n                    byrow = T, nrow = 5)\nci_matrix &lt;- matrix(c(-1, -3, 0, 0, 0), nrow = 5)\n\noutput &lt;- constrOptim(theta = theta_init, f = minimise, grad = NULL, \n                      ui = ui_matrix, ci = ci_matrix)\n\nas.vector(round(output$par, 5))\n\n[1] 2 5 0"
  },
  {
    "objectID": "Monte_Carlo_Prac.html",
    "href": "Monte_Carlo_Prac.html",
    "title": "Monte Carlo Practical",
    "section": "",
    "text": "Plot the following function over the range \\(−5 &lt; x &lt; 5\\):\n\n\\[\nf(x) = e^{\\frac{-x^2}{2}}[sin(2x)]^2\n\\]\n\ngiven_func &lt;- function(x)\n{\n  return(exp((-x^2/2))*(sin(2*x))^2)\n}\n\nx_vals &lt;- seq(-5, 5, length.out = 1000)\ny_vals &lt;- given_func(x_vals)\nnorm_vals &lt;- dnorm(x_vals, mean = 0, sd = 1)\nscaled_norm_vals &lt;- dnorm(x_vals, mean = 0, sd = 1)*3\n\ndata &lt;- data.frame(x = x_vals, y = y_vals, norm = norm_vals, \n                   candidate = scaled_norm_vals)\n\nggplot(data = data) + \n  geom_line(aes(x = x, y = y, color = \"y\")) +\n  scale_color_manual(\n    values = c(\"y\" = \"black\"),\n    labels = c(\"y\" = \"Function Data\")\n  ) +\n  labs(color = \"Distribution\")\n\n\n\n\n\nFind a normal distribution \\((h(x))\\) that qualifies as a candidate distribution (do this graphically by trial and error), i.e. find a constant \\(C\\) such that \\(Ch(x)\\) satisfies the requirements of a candidate distribution.\n\n\nggplot(data = data) + \n  geom_line(aes(x = x, y = y, color = \"y\")) + \n  geom_line(aes(x = x, y = norm, color = \"norm\")) + \n  geom_line(aes(x = x, y = candidate, color = \"candidate\")) +\n  scale_color_manual(\n    values = c(\"y\" = \"black\", \"norm\" = \"red\", \"candidate\" = \"blue\"),\n    labels = c(\"y\" = \"Function Data\", \"norm\" = \"h(x)\", \"candidate\" = \"Ch(x)\")\n  ) +\n  labs(color = \"Distribution\")\n\n\n\n\n\nGenerate N values each from the candidate distribution, and from a \\(U(0,1)\\) distribution.\n\n\nN &lt;- 1000000\ncandidate_values &lt;- rnorm(N, 0, 1)\nuniform_values   &lt;- runif(N, 0, 1)\n\n\nCalculate the ratio:\n\n\\[\ng(x) = \\frac{f(x)}{C \\ h(x)}\n\\]\nfor each of the candidate values generated above. Substitute your constant for \\(C\\). \\(x\\) is the value generated from \\(h(x)\\).\n\n# Guess for C Value\nC &lt;- 3\n\n# Area under the curve\nu &lt;- runif(100000, -4, 4)\nfmean &lt;- mean(given_func(u))\narea &lt;- fmean * 8\nnc &lt;- 1/area                  # Provides a normalising constant for the function\n\ng_x_values &lt;- given_func(candidate_values)/(C*dnorm(candidate_values))\n\n\nAccept \\(x\\) with probability \\(g\\).\n\n\naccepted_indicies &lt;- uniform_values &lt;= g_x_values\naccepted_values   &lt;- candidate_values[accepted_indicies]\n\n\nPlot a histogram of the generated values, and plot the target distribution (\\(f(x)\\)) on top to check.\n\n\nggplot() +\n  geom_histogram(aes(x = accepted_values, y = after_stat(density)),\n    bins = 100,              # Number of bins\n    fill = \"lightblue\",      # Fill colour of bars\n    color = \"black\",         # Border colour of bars\n    alpha = 0.8,             # Transparency\n    boundary = 0,            # Start bins at zero\n    show.legend = FALSE      # Hide the legend\n  ) +\n  stat_function(\n    fun = \\(x) given_func(x)*nc,  # Custom function for the distribution\n    color = \"red\"                 # Line thickness\n  ) +\n  labs(title = \"Histogram of Generated Values\", x = \"Variable\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\nWe note that the function needs a normalising constant so it integrates to 1\n\n\n\n\n\nComment on the proportion of values accepted. Is this acceptable?\n\n\nsum(accepted_indicies)/length(accepted_indicies)\n\n[1] 0.417984\n\n\nProportion accepted \\(\\approx 42\\%\\), this is not preferable for high dimensional datasets.\n\nThe sample generated using the accept-reject method is a random sample generated from the:\n\nTarget Distribution\nCandidate Distribution\nNone of the Above\n\nUse simple MCMC (Metropolis algorithm) to sample from \\(f(x)\\)\n\n\ngiven_func &lt;- function(x)\n{\n  return(exp((-x^2/2))*(sin(2*x))^2)\n}\n\nmetropolis_algorithm &lt;- function(initial, f_func, output_len)\n{\n  x &lt;- numeric(output_len)\n  x[1] &lt;- initial\n  i = 2\n  while (i &lt;= output_len) \n  {\n    # Generate a proposed point\n    proposal &lt;- rnorm(1, mean = (x[i-1]), sd = 1)\n    \n    # Get the acceptance probability\n    acceptance_pr &lt;- min(1, f_func(proposal)/f_func(x[i-1]))\n    \n    # Accept or Reject Candidate\n    if (runif(1) &lt; acceptance_pr) \n    {\n      x[i] &lt;- proposal\n      i    &lt;- i + 1\n    }\n    else\n    {\n      x[i] &lt;- x[i-1]\n      i    &lt;- i + 1\n    }\n  }\n  return(x)\n}\n\nobs_out &lt;- 100000\nsample &lt;- metropolis_algorithm(initial = 0, f_func = given_func, \n                               output_len = obs_out)\n\nggplot() +\n  geom_histogram(aes(x = sample, y = after_stat(density)),\n    bins = 100,              # Number of bins\n    fill = \"lightblue\",      # Fill colour of bars\n    color = \"black\",         # Border colour of bars\n    alpha = 0.8,             # Transparency\n    boundary = 0,            # Start bins at zero\n    show.legend = FALSE      # Hide the legend\n  ) +\n  stat_function(\n    fun = \\(x) given_func(x)*nc,  # Custom function for the distribution\n    color = \"red\"                 # Line thickness\n  ) +\n  labs(title = \"Histogram of Generated Values\", x = \"Variable\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-1-accept-reject-method",
    "href": "Monte_Carlo_Prac.html#question-1-accept-reject-method",
    "title": "Monte Carlo Practical",
    "section": "",
    "text": "Plot the following function over the range \\(−5 &lt; x &lt; 5\\):\n\n\\[\nf(x) = e^{\\frac{-x^2}{2}}[sin(2x)]^2\n\\]\n\ngiven_func &lt;- function(x)\n{\n  return(exp((-x^2/2))*(sin(2*x))^2)\n}\n\nx_vals &lt;- seq(-5, 5, length.out = 1000)\ny_vals &lt;- given_func(x_vals)\nnorm_vals &lt;- dnorm(x_vals, mean = 0, sd = 1)\nscaled_norm_vals &lt;- dnorm(x_vals, mean = 0, sd = 1)*3\n\ndata &lt;- data.frame(x = x_vals, y = y_vals, norm = norm_vals, \n                   candidate = scaled_norm_vals)\n\nggplot(data = data) + \n  geom_line(aes(x = x, y = y, color = \"y\")) +\n  scale_color_manual(\n    values = c(\"y\" = \"black\"),\n    labels = c(\"y\" = \"Function Data\")\n  ) +\n  labs(color = \"Distribution\")\n\n\n\n\n\nFind a normal distribution \\((h(x))\\) that qualifies as a candidate distribution (do this graphically by trial and error), i.e. find a constant \\(C\\) such that \\(Ch(x)\\) satisfies the requirements of a candidate distribution.\n\n\nggplot(data = data) + \n  geom_line(aes(x = x, y = y, color = \"y\")) + \n  geom_line(aes(x = x, y = norm, color = \"norm\")) + \n  geom_line(aes(x = x, y = candidate, color = \"candidate\")) +\n  scale_color_manual(\n    values = c(\"y\" = \"black\", \"norm\" = \"red\", \"candidate\" = \"blue\"),\n    labels = c(\"y\" = \"Function Data\", \"norm\" = \"h(x)\", \"candidate\" = \"Ch(x)\")\n  ) +\n  labs(color = \"Distribution\")\n\n\n\n\n\nGenerate N values each from the candidate distribution, and from a \\(U(0,1)\\) distribution.\n\n\nN &lt;- 1000000\ncandidate_values &lt;- rnorm(N, 0, 1)\nuniform_values   &lt;- runif(N, 0, 1)\n\n\nCalculate the ratio:\n\n\\[\ng(x) = \\frac{f(x)}{C \\ h(x)}\n\\]\nfor each of the candidate values generated above. Substitute your constant for \\(C\\). \\(x\\) is the value generated from \\(h(x)\\).\n\n# Guess for C Value\nC &lt;- 3\n\n# Area under the curve\nu &lt;- runif(100000, -4, 4)\nfmean &lt;- mean(given_func(u))\narea &lt;- fmean * 8\nnc &lt;- 1/area                  # Provides a normalising constant for the function\n\ng_x_values &lt;- given_func(candidate_values)/(C*dnorm(candidate_values))\n\n\nAccept \\(x\\) with probability \\(g\\).\n\n\naccepted_indicies &lt;- uniform_values &lt;= g_x_values\naccepted_values   &lt;- candidate_values[accepted_indicies]\n\n\nPlot a histogram of the generated values, and plot the target distribution (\\(f(x)\\)) on top to check.\n\n\nggplot() +\n  geom_histogram(aes(x = accepted_values, y = after_stat(density)),\n    bins = 100,              # Number of bins\n    fill = \"lightblue\",      # Fill colour of bars\n    color = \"black\",         # Border colour of bars\n    alpha = 0.8,             # Transparency\n    boundary = 0,            # Start bins at zero\n    show.legend = FALSE      # Hide the legend\n  ) +\n  stat_function(\n    fun = \\(x) given_func(x)*nc,  # Custom function for the distribution\n    color = \"red\"                 # Line thickness\n  ) +\n  labs(title = \"Histogram of Generated Values\", x = \"Variable\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\nWe note that the function needs a normalising constant so it integrates to 1\n\n\n\n\n\nComment on the proportion of values accepted. Is this acceptable?\n\n\nsum(accepted_indicies)/length(accepted_indicies)\n\n[1] 0.417984\n\n\nProportion accepted \\(\\approx 42\\%\\), this is not preferable for high dimensional datasets.\n\nThe sample generated using the accept-reject method is a random sample generated from the:\n\nTarget Distribution\nCandidate Distribution\nNone of the Above\n\nUse simple MCMC (Metropolis algorithm) to sample from \\(f(x)\\)\n\n\ngiven_func &lt;- function(x)\n{\n  return(exp((-x^2/2))*(sin(2*x))^2)\n}\n\nmetropolis_algorithm &lt;- function(initial, f_func, output_len)\n{\n  x &lt;- numeric(output_len)\n  x[1] &lt;- initial\n  i = 2\n  while (i &lt;= output_len) \n  {\n    # Generate a proposed point\n    proposal &lt;- rnorm(1, mean = (x[i-1]), sd = 1)\n    \n    # Get the acceptance probability\n    acceptance_pr &lt;- min(1, f_func(proposal)/f_func(x[i-1]))\n    \n    # Accept or Reject Candidate\n    if (runif(1) &lt; acceptance_pr) \n    {\n      x[i] &lt;- proposal\n      i    &lt;- i + 1\n    }\n    else\n    {\n      x[i] &lt;- x[i-1]\n      i    &lt;- i + 1\n    }\n  }\n  return(x)\n}\n\nobs_out &lt;- 100000\nsample &lt;- metropolis_algorithm(initial = 0, f_func = given_func, \n                               output_len = obs_out)\n\nggplot() +\n  geom_histogram(aes(x = sample, y = after_stat(density)),\n    bins = 100,              # Number of bins\n    fill = \"lightblue\",      # Fill colour of bars\n    color = \"black\",         # Border colour of bars\n    alpha = 0.8,             # Transparency\n    boundary = 0,            # Start bins at zero\n    show.legend = FALSE      # Hide the legend\n  ) +\n  stat_function(\n    fun = \\(x) given_func(x)*nc,  # Custom function for the distribution\n    color = \"red\"                 # Line thickness\n  ) +\n  labs(title = \"Histogram of Generated Values\", x = \"Variable\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-2-probability-integral-transform",
    "href": "Monte_Carlo_Prac.html#question-2-probability-integral-transform",
    "title": "Monte Carlo Practical",
    "section": "Question 2: Probability Integral Transform",
    "text": "Question 2: Probability Integral Transform\n\nGenerate values from an exponential distribution with\\(\\lambda = 2\\), using the probability integral transform.\n\n\\[\nf(x) = \\lambda e^{-\\lambda x}\n\\]\n\\[\n\\begin{aligned}\nf(x) &= \\lambda e^{-\\lambda x} \\\\\nF(x) &= P(X \\le x) = \\int_0^x \\lambda e^{-\\lambda x} dx = - e^{-\\lambda x} |^x_0 = 1 - e^{-\\lambda x} \\\\\n\\implies x &= - \\frac{\\log(1 - u)}{\\lambda}\n\\end{aligned}\n\\]\n\ninv_exp &lt;- function(unif, lambda = 2)\n{\n  return(-(log(1-unif)/lambda))\n}\n\nN &lt;- 100000\nunif_obs   &lt;- runif(N, 0, 1)\nPIT_sample &lt;- inv_exp(unif_obs)\n\n\nMake appropriate plots to check that this has worked.\n\n\nggplot() +\n  geom_histogram(aes(x = PIT_sample, y = after_stat(density)),\n    bins = 100,              # Number of bins\n    fill = \"lightblue\",      # Fill colour of bars\n    color = \"black\",         # Border colour of bars\n    alpha = 0.8,             # Transparency\n    boundary = 0            # Start bins at zero\n  ) +\n  stat_function(\n    fun = \\(x) dexp(x, rate = 2),  # Custom function for the distribution\n    color = \"red\"                 # Line thickness\n  ) +\n  labs(title = \"Histogram of Generated Values\", x = \"Variable\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-3-importance-sampling",
    "href": "Monte_Carlo_Prac.html#question-3-importance-sampling",
    "title": "Monte Carlo Practical",
    "section": "Question 3: Importance Sampling",
    "text": "Question 3: Importance Sampling\n\nEstimate, using importance sampling:\n\n\\[\n\\int_0^1\\frac{e^{-x}}{1+x^2}dx\n\\]\nCompare your estimates and their standard errors using the following importance function:\n\\[\n\\begin{aligned}\nf_0(x) &= 1, \\qquad \\qquad & 0&lt;x&lt;1 \\\\ \\\\\nf_1(x) &= e^{-x}, \\qquad \\qquad & 0&lt;x&lt; \\infty \\\\ \\\\\nf_3(x) &= \\frac{e^{-x}}{1-e^{-1}}, \\qquad \\qquad & 0&lt;x&lt;1\n\\end{aligned}\n\\]\nOne can generate from \\(f_3(x)\\) using inverse transform sampling, and from \\(f_1(x)\\) using the rexp() random number generator. We then use all these as if they are the distribution and we are integrating over the function \\(f/f_i\\).\nWe define \\(h(x) = \\frac{e^{-x}}{1+x^2}\\), \\(f(x)=1\\) and \\(p(x) = 1\\).\n\n## First we define h(x)\nh_x_function &lt;- function(x)\n{\n  return(exp(-x)/(1+x^2))\n}\n\nf_0_function &lt;- function(x)\n{\n  return(1)\n}\n\nf_1_function &lt;- function(x)\n{\n  return(exp(-x))\n}\n\nf_3_function &lt;- function(x)\n{\n  return(exp(-x)/(1-exp(-1)))\n}\n\nN &lt;- 100000\nuniform_values &lt;- runif(N, 0, 1)\n\n# Getting true values\nint_true   &lt;- integrate(h_x_function, lower = 0, upper = 1)\ntrue_value &lt;- int_true$value\ntrue_err   &lt;- int_true$abs.error\n\n# Need to generate the x values from the inverses first\n# Then need to generate the weighted values based of f_i chosen\n# Then need to take the means (don't forget domain!!)\n\nx_0 &lt;- uniform_values\nx_1 &lt;- -log(1 - uniform_values)\nx_3 &lt;- -log(1 - (uniform_values)*(1 - exp(-1)))\n\nratio_0 &lt;- h_x_function(x_0) / 1\nratio_1 &lt;- h_x_function(x_1) / f_1_function(x_1)\nratio_3 &lt;- h_x_function(x_3) / f_3_function(x_3)\n\nmean_0 &lt;- mean(ratio_0)\nmean_1 &lt;- mean(ratio_1)\nmean_3 &lt;- mean(ratio_3)\n\nvar_0  &lt;- var(ratio_0)/N\nvar_1  &lt;- var(ratio_1)/N\nvar_3  &lt;- var(ratio_3)/N\n\nsd_err_0 &lt;- sqrt(var_0)\nsd_err_1 &lt;- sqrt(var_1)\nsd_err_3 &lt;- sqrt(var_3)\n\nestimates &lt;- c(mean_0, mean_1, mean_3, true_value)\nstd_err   &lt;- c(sd_err_0, sd_err_1, sd_err_3, true_err)\nfunctions &lt;- c(\"Uniform\", \"f1(x)\", \"f3(x)\", \"True Value\")\n\ndata.frame(functions, estimates, std_err) |&gt;\n  kable(digits = 4, col.names = c(\"Importance Functions\", \"Estimates\", \n                                         \"Std. Errors\"))\n\n\n\n\nImportance Functions\nEstimates\nStd. Errors\n\n\n\n\nUniform\n0.5259\n8e-04\n\n\nf1(x)\n0.6226\n1e-03\n\n\nf3(x)\n0.5252\n3e-04\n\n\nTrue Value\n0.5248\n0e+00"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-4-random-sums",
    "href": "Monte_Carlo_Prac.html#question-4-random-sums",
    "title": "Monte Carlo Practical",
    "section": "Question 4: Random Sums",
    "text": "Question 4: Random Sums\n\nUse Monte Carlo methods to estimate the mean, variance, and P(S &gt; c) for the random sum of exponentials (example in slides). Also find the standard errors (Monte Carlo error) of the mean and the probability. rnbinom() generates values from a negative binomial distribution.\n\n\np &lt;- 0.5   # use these parameters for the negative binomial NB(p,r)\nr &lt;- 20\nmu &lt;- r*(1-p)/p\n\nx0 &lt;- 5       # use these settings\nM &lt;- 10000    # number of Monte Carlo simulations\nc &lt;- 100\n\nS_gt_c &lt;- 0\nSUMS   &lt;- numeric(M)\n\nfor (i in 1:M)\n{\n  nbinom_relisation &lt;- rnbinom(1, size = r, prob = p)\n  exp_mean &lt;- x0\n  S &lt;- 0\n  for (j in 2:nbinom_relisation) \n  {\n    x_value &lt;- rexp(1, 1/exp_mean)\n    S &lt;- S + x_value\n    exp_mean &lt;- x_value\n  }\n\n  SUMS[i] &lt;- S\n  if (S &gt; c) S_gt_c = S_gt_c + 1\n}\n\n# Estimate for E[S]\nest_S &lt;- mean(SUMS)\n\n# Estimate of SE of S\nse_S  &lt;- sqrt(var(SUMS)/M)\n\n# Pr(S &gt; c) Estimate\np_est &lt;- S_gt_c / M\n\n# Estimate of SE for above probability estimate\nse_p_est &lt;- sqrt(p_est * (1-p_est) / M)\n\ndata.frame(est_S, se_S, p_est, se_p_est) |&gt;\n  kable(digits = 4, col.names = c(\"E(S)\", \"SE(S)\", \"Pr(S &gt; c)\"\n                                  , \"SE(Pr(S &gt; c))\"))\n\n\n\n\nE(S)\nSE(S)\nPr(S &gt; c)\nSE(Pr(S &gt; c))\n\n\n\n\n66.9905\n3.6497\n0.1017\n0.003"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-5-the-random-number-generator-randu",
    "href": "Monte_Carlo_Prac.html#question-5-the-random-number-generator-randu",
    "title": "Monte Carlo Practical",
    "section": "Question 5: The Random Number Generator RANDU",
    "text": "Question 5: The Random Number Generator RANDU\n\\[\nx_i = 65539x_{i-1} \\ \\text{mod} \\ \\ 2^{31}\n\\]\n\nCan you find a problem with the following random number generator? What is it\n\n\nx0 &lt;- sample(1:1000000, size = 1)   # randomly choose a starting value between\n\nx &lt;- c()         # prepare vector x, into which we are going to put values\nx[1] &lt;- 65539 * x0 %% 2^31          # first value\n\nfor(i in 2:10000) \n{\n  x[i] &lt;- 65539 * x[i - 1] %% 2^31     # uniform random number generator called RANDU\n}\n\nset1 &lt;- seq(1, 10000, by = 3)     # create indices of three sets,\nset2 &lt;- seq(2, 10000, by = 3)     # every 3rd, starting from 2\nset3 &lt;- seq(3, 10000, by = 3)     # every 3rd, starting from 3\nx1 &lt;- x[set1]/1e12       # subset 1 (at lag 2)\nx2 &lt;- x[set2]/1e12       # subset 2 (at lag 1)\nx3 &lt;- x[set3]/1e12       # subset 3\n\nsetupKnitr(autoprint = TRUE)\nplot3d(x1, x2, x3, col = \"red\", cex = 0.7, size = 5)\nview3d(theta = 45, phi = 45, fov = 45, zoom = 0.9)  #Initial view settings\n\n\n\n\n\nBy turning the plot it can be observed that all points fall into one of 15 planes. This means the random number generator is unable to completely cover a 3-dimensional space and there is correlation between consecutive values. Good random number generators must be able to pass all tests of independence and there should be no pattern or structure. RANDU was the most popular generator until this problem was discovered!!"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-6-monte-carlo-integration",
    "href": "Monte_Carlo_Prac.html#question-6-monte-carlo-integration",
    "title": "Monte Carlo Practical",
    "section": "Question 6: Monte Carlo Integration",
    "text": "Question 6: Monte Carlo Integration\n\nIntegrate and report the Monte Carlo Error of your estimate:\n\n\\[\n\\int^3_{0.8} \\frac{1}{1 + sinh(2x) \\ . \\ log(x)^2}\n\\]\n\ngiven_func &lt;- function(x)\n{\n  return(1/(1 + sinh(2*x) * log(x)^2))\n}\n\n# True Values:\ntrue_vals &lt;- integrate(given_func, lower = 0.8, upper = 3)\n\nN &lt;- 1000000\nunif_vals &lt;- runif(N, 0.8, 3)\n\ngiven_of_unif &lt;- given_func(unif_vals)\nint.est  &lt;- mean(given_of_unif) * (3 - 0.8)\nint.var  &lt;- (3 - 0.8)^2 * var(given_of_unif) / N\nint.serr &lt;- sqrt(int.var)\n\nkable(data.frame(c(int.est, true_vals$value), c(int.serr, true_vals$abs.error), \n                 row.names = c(\"Estimate\", \"True Value\")),\n      digits = 4, col.names = c(\"Value\", \"Std. Error\"))\n\n\n\n\n\nValue\nStd. Error\n\n\n\n\nEstimate\n0.6787\n8e-04\n\n\nTrue Value\n0.6768\n1e-04"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-7-estimate-pi",
    "href": "Monte_Carlo_Prac.html#question-7-estimate-pi",
    "title": "Monte Carlo Practical",
    "section": "Question 7: Estimate \\(\\pi\\)",
    "text": "Question 7: Estimate \\(\\pi\\)\n\nFind a Monte Carlo estimate of \\(\\pi\\). What is the error with \\(N = 1000\\) generated points?\n\n\n## Approach 1: Integration Imitation:\nquarter_circle_func &lt;- function(x)\n{\n  return(sqrt(1 - x^2))\n}\n\nN &lt;- 1000\nunif_vals &lt;- runif(N, 0, 1)\nvalues    &lt;- quarter_circle_func(unif_vals)\n\n# Use the fact that a circle is just 4 quarters\npi.estimate &lt;- mean(values)*4\npi.est.serr &lt;- sqrt(var(values)/N)\n\n## Approach 2: Scaled Proportion\nN &lt;- 1000\nx &lt;- runif(N)\ny &lt;- runif(N)\n\nin.circle &lt;- sum(x^2 + y^2 &lt; 1) \np.hat     &lt;- in.circle / N\npi.hat    &lt;- p.hat * 4\n# Note is from a binomial distribution - thus binomial variance\npi.hat.se &lt;- sqrt((N * p.hat * (1 - p.hat) / N^2) * 16)\n\nkable(data.frame(c(pi.estimate, pi.est.serr), c(pi.hat, pi.hat.se), \n                 row.names = c(\"MC Estimate\", \n                               \"Scaled Proportion Estimate\")),\n      digits = 4, col.names = c(\"Value\", \"Std. Error\"))\n\n\n\n\n\nValue\nStd. Error\n\n\n\n\nMC Estimate\n3.1249\n3.0880\n\n\nScaled Proportion Estimate\n0.0070\n0.0531"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-8-discrete-inverse-transform-sampling",
    "href": "Monte_Carlo_Prac.html#question-8-discrete-inverse-transform-sampling",
    "title": "Monte Carlo Practical",
    "section": "Question 8: Discrete Inverse Transform Sampling",
    "text": "Question 8: Discrete Inverse Transform Sampling\n\nUsing only the runif() random number generator in R, generate random outcomes from the following distribution: \\[\np(x) =\n\\begin{cases}\n0.4, \\qquad &x = \\text{turn left} \\\\\n0.5, \\qquad &x = \\text{turn right} \\\\\n0.1, \\qquad &x = \\text{stay in place} \\\\\n0, \\qquad &x = \\text{otherwise} \\\\\n\\end{cases}\n\\]\n\n\nN &lt;- 10000\nunif_vals &lt;- runif(N, 0, 1)\nx &lt;- ifelse(unif_vals &lt;= 0.4, \"left\", ifelse(unif_vals &lt;= 0.9, \"right\", \"stay\"))\n\nggplot() +\n  geom_bar(aes(x=x, y = after_stat(count)/N),  \n           color = \"black\",\n           alpha = 0.8, \n           fill = \"lightblue\") +\n  labs(x = \"Movement Decision\", y = \"Count Proportion\") +\n  theme_minimal()"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-9-antithetic-sampling",
    "href": "Monte_Carlo_Prac.html#question-9-antithetic-sampling",
    "title": "Monte Carlo Practical",
    "section": "Question 9: Antithetic Sampling",
    "text": "Question 9: Antithetic Sampling\n\nUse Monte Carlo integration to estimate the following integral, find the MC error with N = 1000, then use antithetic sampling while comparing estimates and errors:\n\n\\[\n\\int_0^1f(x) \\ dx \\ , \\qquad \\qquad \\text{where} \\  f(x) = x^2\n\\]\n\ntrue_vals &lt;- integrate(\\(x) x^2, 0, 1)\n\nN &lt;- 1000\nunif_vals   &lt;- runif(N, 0, 1)\nestimate_MC &lt;- mean(unif_vals^2)\nMC_sterr    &lt;- sqrt(var(unif_vals^2)/N)\n\nunif_vals_2 &lt;- runif(N/2, 0, 1)\nanti_unifs &lt;- (unif_vals_2^2 + (1 - unif_vals_2)^2)/2\nantithetic_est   &lt;- mean(anti_unifs)\nantithetic_sterr &lt;- sqrt(var(anti_unifs)/(N/2))\n\n\nkable(data.frame(c(true_vals$value, estimate_MC, antithetic_est), \n                 c(true_vals$abs.error, MC_sterr, antithetic_sterr), \n                 row.names = c(\"Intergration\", \"MC Estimate\", \n                               \"Scaled Proportion Estimate\")),\n      digits = 4, col.names = c(\"Value\", \"Std. Error\"))\n\n\n\n\n\nValue\nStd. Error\n\n\n\n\nIntergration\n0.3333\n0.0000\n\n\nMC Estimate\n0.3277\n0.0093\n\n\nScaled Proportion Estimate\n0.3332\n0.0033"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#question-10-mcmc",
    "href": "Monte_Carlo_Prac.html#question-10-mcmc",
    "title": "Monte Carlo Practical",
    "section": "Question 10: MCMC",
    "text": "Question 10: MCMC\n\nUse the Metropolis algorithm to sample from \\(f(x)\\) using \\(g(x)\\) : \\(X \\sim U(−1,1)\\) as proposal distribution and N = 5000. Illustrate your results.\n\n\\[\nf(x) = 10e^{-4(x+4)^2} + 3e^{-0.2(x+1)^2} + e^{-2(x-5)^2}\n\\]\n\ngiven_func &lt;- function(x)\n{\n  return(10*exp(-4*(x+4)^2) + 3*exp(-0.2*(x+1)^2) + exp(-2*(x-5)^2))\n}\n\nmetropolis_algorithm &lt;- function(initial, f_func, output_len)\n{\n  x &lt;- numeric(output_len)\n  x[1] &lt;- initial\n  i = 2\n  while (i &lt;= output_len) \n  {\n    # Generate a proposed point\n    proposal &lt;- x[i-1] + runif(1, -1, 1)\n    \n    # Get the acceptance probability\n    acceptance_pr &lt;- f_func(proposal)/f_func(x[i-1])\n    \n    # Accept or Reject Candidate\n    if (runif(1) &lt;= acceptance_pr) \n    {\n      x[i] &lt;- proposal\n      i    &lt;- i + 1\n    }\n    else\n    {\n      x[i] &lt;- x[i-1]\n      i    &lt;- i + 1\n    }\n  }\n  return(x)\n}\n\n# Checking distribution\n# x_vals &lt;- seq(-10, 10, length.out = 10000)\n# y_vals &lt;- given_func(x_vals)\n# plot(x_vals, y_vals)\n\n# Area under the curve\nu &lt;- runif(10000, -10, 10)\nfmean &lt;- mean(given_func(u))\narea &lt;- fmean * 20\nnc &lt;- 1/area                  # Provides a normalising constant for the function\n\nsample &lt;- metropolis_algorithm(0, given_func, 500000)\n\nggplot() +\n  geom_histogram(aes(x = sample, y = after_stat(density)),\n    bins = 100,              # Number of bins\n    fill = \"lightblue\",      # Fill colour of bars\n    color = \"black\",         # Border colour of bars\n    alpha = 0.8,             # Transparency\n    boundary = 0,            # Start bins at zero\n    show.legend = FALSE      # Hide the legend\n  ) +\n  stat_function(\n    fun = \\(x) given_func(x)*nc,  # Custom function for the distribution\n    color = \"red\"                 # Line thickness\n  ) +\n  labs(title = \"Histogram of Generated Values\", x = \"Variable\", y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "Monte_Carlo_Prac.html#things-to-ask-birgit",
    "href": "Monte_Carlo_Prac.html#things-to-ask-birgit",
    "title": "Monte Carlo Practical",
    "section": "Things to ask Birgit:",
    "text": "Things to ask Birgit:\n\nWhy does keeping the previous value if the new one is rejected mean the distribution tends to the shape? (Q1.9) (Q10)\nIs this essential to the algorithm? (Not doing so means it often doesn’t converge?) (Q1.9) (Q10)\nIs the proportionality variance alike binomial because of the nature of it? (Q7)"
  },
  {
    "objectID": "Bootstrap_Prac.html",
    "href": "Bootstrap_Prac.html",
    "title": "Bootstrapping Practical",
    "section": "",
    "text": "library(MASS)\nlibrary(ggplot2)\nlibrary(knitr)\n\n# Generate a uniform using sample\nunif_1 &lt;- sample(0:99, size = 50, replace = TRUE)\n\n# Sample from this without replacement\nunif_2 &lt;- sample(unif_1, size = 10, replace = FALSE)\n\n# Sample with replacement multiple times\nunif_3 &lt;- replicate(10, sample(unif_1, size = 50, replace = TRUE))"
  },
  {
    "objectID": "Bootstrap_Prac.html#question-1-median-speed-of-galaxies",
    "href": "Bootstrap_Prac.html#question-1-median-speed-of-galaxies",
    "title": "Bootstrapping Practical",
    "section": "Question 1: Median speed of galaxies",
    "text": "Question 1: Median speed of galaxies\n\ngal &lt;- galaxies\nggplot() +\n  geom_histogram(aes(x = gal, y = after_stat(density)),\n                 bins = 20,\n                 alpha = 0.8,\n                 fill = \"lightblue\",\n                 colour = \"black\")\n\n\n\ntobs_median &lt;- median(gal)\n\n\nSet the number of bootstrap samples to B = 10000.\n\n\nB &lt;- 10000\n\n\nTake B Bootstrap samples, each time calculating the median speed, store in tboot.\n\n\ntboot &lt;- replicate(B, median(sample(gal, length(gal), replace = TRUE)))\n\n\nWhat is mean(tboot) - tobs_median?\n\n\nmean(tboot) - tobs_median\n\n[1] 41.7576\n\n\n\nWhat is sd(tboot)? Will this decrease if B is increased? Explain. What exactly will this value tell us?\n\n\nse &lt;- sd(tboot)\nse\n\n[1] 513.6926\n\n\nThis represents the standard deviation of the bootstrap estimate of the median speed. A measure of variability in estimated median across the B bootstrap samples. It is unlikely to reduce as B increases as it is about the variability in the statistic as opposed to how many samples you take. By increasing B, we may obtain a more precise statistic, however this doesn’t mean the variability will be decreased.\n\nPlot the bootstrap distribution (histogram), indicate tobs_median on this.\n\n\nggplot() +\n  geom_histogram(aes(x = tboot, y = after_stat(density)),\n                 bins = 20,\n                 colour = \"black\",\n                 fill = \"lightblue\",\n                 alpha = 0.8) +\n  geom_vline(xintercept = tobs_median, colour = \"red\", size = 1, linetype = \"dashed\") +\n  geom_text(aes(x = median(tboot), y = 9e-4, label = \"tobs_median\"), colour = \"red\", nudge_x = 300)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nCalculate a percentile and a basic bootstrap confidence interval for median galaxy speed. Compare.\n\n\n# For the 95% confidence interval, we use the 2.5th and 97.5th percentiles\nmed_percentile_ci &lt;- quantile(tboot, c(0.025, 0.975))\n\n# Calculate the 95% bootstrap confidence interval (percentiles)\nmed_bootstrap_ci &lt;- 2*c(tobs_median, tobs_median) - quantile(tboot, c(0.975, 0.025))\n\nkable(data.frame(col1 = c(med_percentile_ci[1], med_bootstrap_ci[1]),\n                 col2 = c(med_percentile_ci[2], med_bootstrap_ci[2]), \n                 row.names = c(\"Percentile\", \"Bootstrap\")), \n      col.names = c(\"2.5%\", \"97.5%\"), digits = 4)\n\n\n\n\n\n2.5%\n97.5%\n\n\n\n\nPercentile\n20170.5\n22053.0\n\n\nBootstrap\n19614.0\n21496.5\n\n\n\n\n\nThe basic bootstrap confidence interval is much smaller than the percentile confidence interval.\n\nCalculate percentile, basic bootstrap and bootstrap T confidence intervals for mean galaxy speed. Compare.\n\n\nboot.funct &lt;- function()\n{\n  boot.sample &lt;- sample(gal, replace = TRUE)\n  return(c(mean(boot.sample), sd(boot.sample)))\n}\n\nboot.stats &lt;- replicate(B, boot.funct())\ntboot_mean &lt;- boot.stats[1,]                            # Means for samples \ntboot_sds  &lt;- boot.stats[2,]                            # SDs for samples \ntobs_mean  &lt;- mean(gal)                                 # Mean of original\ntsd        &lt;- sd(gal)                                   # SD of original\nn          &lt;- length(gal)\nboot.ts    &lt;- (tboot_mean - tobs_mean)/(tboot_sds / sqrt(n)) # T stats for samples\n\n# Calculate the 95% bootstrap confidence interval (percentiles)\npercentile_ci &lt;- quantile(tboot_mean, c(0.025, 0.975))\n\n# Calculate the 95% bootstrap confidence interval (percentiles)\nbootstrap_ci &lt;- 2*c(tobs_mean, tobs_mean) - quantile(tboot_mean, c(0.975, 0.025))\n\n# Calculate the 95% T confidence interval (percentiles)\nt_ci &lt;- c(tobs_mean, tobs_mean) - (tsd/sqrt(n)) * quantile(boot.ts, c(0.975, 0.025))\n\nkable(data.frame(col1 = c(percentile_ci[1], bootstrap_ci[1], t_ci[1]),\n                 col2 = c(percentile_ci[2], bootstrap_ci[2], t_ci[2]), \n                 row.names = c(\"Percentile CI\", \"Bootstrap CI\", \"T CI\")), \n      col.names = c(\"2.5%\", \"97.5%\"), digits = 4)\n\n\n\n\n\n2.5%\n97.5%\n\n\n\n\nPercentile CI\n19832.50\n21804.78\n\n\nBootstrap CI\n19851.56\n21823.84\n\n\nT CI\n19769.98\n21795.47\n\n\n\n\n\nAll three of these confidence intervals are relatively similar. This is since the bootstrap distribution is relatively symmetric and means are unbiased."
  },
  {
    "objectID": "Bootstrap_Prac.html#question-2-galaxies-again",
    "href": "Bootstrap_Prac.html#question-2-galaxies-again",
    "title": "Bootstrapping Practical",
    "section": "Question 2: Galaxies Again",
    "text": "Question 2: Galaxies Again\n\nCompare with your values for bias, standard error, percentile and basic bootstrap CI\n\n\nlibrary(boot)\n\nbt.smpls &lt;- boot(gal, function(x, i) median(x[i]), R = 3000)         \n\n# see Venables and Ripley, pg.134\n# take 3000 bootstrap samples, returns the 3000 medians of these\n\nbt.smpls\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = gal, statistic = function(x, i) median(x[i]), R = 3000)\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1*  20833.5 38.01783    510.8616\n\nboot.ci(bt.smpls, type = c(\"norm\", \"basic\", \"perc\", \"bca\"))    # all sorts of confidence intervals\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 3000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bt.smpls, type = c(\"norm\", \"basic\", \"perc\", \n    \"bca\"))\n\nIntervals : \nLevel      Normal              Basic         \n95%   (19794, 21797 )   (19614, 21492 )  \n\nLevel     Percentile            BCa          \n95%   (20175, 22053 )   (20172, 21960 )  \nCalculations and Intervals on Original Scale\n\nhelp(boot.ci)          # bca gives a bias corrected and accelerated \n                       # (improved) percentile interval\ncat(\"Normal CI:\", c(tobs_median - 1.96 * se, tobs_median + 1.96 * se), \"\\n\")\n\nNormal CI: 19826.66 21840.34 \n\ncat(\"Pecentile interval:\", med_percentile_ci, \"\\n\")\n\nPecentile interval: 20170.5 22053 \n\ncat(\"Pecentile interval:\", med_bootstrap_ci, \"\\n\")\n\nPecentile interval: 19614 21496.5"
  },
  {
    "objectID": "Bootstrap_Prac.html#question-3-regression-problem",
    "href": "Bootstrap_Prac.html#question-3-regression-problem",
    "title": "Bootstrapping Practical",
    "section": "Question 3: Regression Problem",
    "text": "Question 3: Regression Problem\nFor this question use the airquality data (airquality R data set).\nFor the correlation between ozone and temperature, find an estimate of SE, bias and construct a confidence interval. Use a nonparametric bootstrap.\n\ndf &lt;- as.data.frame(airquality[, c(\"Temp\", \"Ozone\")]) # Only necessary cols\ndf &lt;- na.omit(df)    # Removes NA's\nn  &lt;- nrow(df)       # Number of obs in dataset\n\ncorrelations &lt;- function(n, df)\n{\n  cases &lt;- sample(1:n, replace = TRUE)\n  booti &lt;- df[cases, ]\n  return(cor(booti[1], booti[2]))\n}\n\nB &lt;- 1000\ncorr.sample &lt;- cor(df$Temp, df$Ozone)\nboot.corrs  &lt;- replicate(B, correlations(n, df))\n\nggplot() +\n  geom_histogram(aes(x = boot.corrs, y = after_stat(density)),\n                 colour = \"black\",\n                 fill = \"lightblue\",\n                 alpha = 0.8,\n                 bins = 20,\n                 ) +\n  theme_minimal()\n\n\n\n# Percentile Interval\npercentile_ci &lt;- quantile(boot.corrs, c(0.025, 0.975))\n\n# Basic Bootstrap Interval\nbootstrap_ci  &lt;- 2*c(corr.sample, corr.sample) - \n                  quantile(boot.corrs, c(0.975, 0.025))\n\n# Bias\nbias.boot &lt;- mean(boot.corrs) - corr.sample\n\n## Standard Error\nse.boot &lt;- sd(boot.corrs)\n\nkable(data.frame(col1 = c(percentile_ci[1], bootstrap_ci[1]),\n                 col2 = c(percentile_ci[2], bootstrap_ci[2]), \n                 row.names = c(\"Percentile CI\", \"Bootstrap CI\")), \n      col.names = c(\"2.5%\", \"97.5%\"), digits = 4, \n      caption = \"Non-Parametric Bootstrapping Confidence Intervals\")\n\n\nNon-Parametric Bootstrapping Confidence Intervals\n\n\n\n2.5%\n97.5%\n\n\n\n\nPercentile CI\n0.6082\n0.7878\n\n\nBootstrap CI\n0.6089\n0.7885\n\n\n\n\n\nExtra: Use semi-parametric bootstrapping to construct a confidence interval for the correlation.\n\n# Get sample correlation\ncorr.sample &lt;- cor(df$Temp, df$Ozone)\nn &lt;- length(df$Temp)\n\n# Fit linear regression model\nmodel &lt;- lm(Ozone ~ Temp, data = df)\n\n# Get the residuals from the fitted model\nresiduals &lt;- model$residuals\n\n# Get the predicted values (fitted values) from the model\nfitted_values &lt;- model$fitted.values\n\npar_boot_correlations &lt;- function(n, df, residuals, fitted_values)\n{\n  bootstrap_residuals &lt;- sample(residuals, size = n, replace = TRUE)\n  \n  bootstrap_ozone &lt;- fitted_values + bootstrap_residuals\n  \n  return(cor(df$Temp, bootstrap_ozone))\n}\n\nB &lt;- 1000\ncorr.sample &lt;- cor(df$Temp, df$Ozone)\nboot.corrs  &lt;- replicate(B, par_boot_correlations(n, df, \n                                                  residuals, fitted_values))\n\n# Percentile Interval\npercentile_ci &lt;- quantile(boot.corrs, c(0.025, 0.975))\n\n# Basic Bootstrap Interval\nbootstrap_ci  &lt;- 2*c(corr.sample, corr.sample) - \n                  quantile(boot.corrs, c(0.975, 0.025))\n\nkable(data.frame(col1 = c(percentile_ci[1], bootstrap_ci[1]),\n                 col2 = c(percentile_ci[2], bootstrap_ci[2]), \n                 row.names = c(\"Percentile CI\", \"Bootstrap CI\")), \n      col.names = c(\"2.5%\", \"97.5%\"), digits = 4, \n      caption = \"Semi-Parametric Bootstrapping Confidence Intervals\")\n\n\nSemi-Parametric Bootstrapping Confidence Intervals\n\n\n\n2.5%\n97.5%\n\n\n\n\nPercentile CI\n0.5823\n0.8037\n\n\nBootstrap CI\n0.5930\n0.8145\n\n\n\n\n\nExtra: Use parametric bootstrapping to construct a confidence interval for the correlation.\n\n# Get sample correlation\ncorr.sample &lt;- cor(df$Temp, df$Ozone)\nn &lt;- length(df$Temp)\n\n# Fit linear regression model\nmodel &lt;- lm(Ozone ~ Temp, data = df)\n\n# Get the residuals from the fitted model\nresiduals &lt;- model$residuals\n\n# Get the predicted values (fitted values) from the model\nfitted_values &lt;- model$fitted.values\n\n# Variance of the residuals\nvar &lt;- var(residuals)\n\npar_boot_correlations &lt;- function(n, df, var, fitted_values)\n{\n  sample_norm &lt;- rnorm(n, mean = 0, sd = sqrt(var))\n  bootstrap_residuals &lt;- sample(sample_norm, size = n, replace = TRUE)\n  \n  bootstrap_ozone &lt;- fitted_values + bootstrap_residuals\n  \n  return(cor(df$Temp, bootstrap_ozone))\n}\n\nB &lt;- 1000\ncorr.sample &lt;- cor(df$Temp, df$Ozone)\nboot.corrs  &lt;- replicate(B, par_boot_correlations(n, df, \n                                                  var, fitted_values))\n\n# Percentile Interval\npercentile_ci &lt;- quantile(boot.corrs, c(0.025, 0.975))\n\n# Basic Bootstrap Interval\nbootstrap_ci  &lt;- 2*c(corr.sample, corr.sample) - \n                  quantile(boot.corrs, c(0.975, 0.025))\n\nkable(data.frame(col1 = c(percentile_ci[1], bootstrap_ci[1]),\n                 col2 = c(percentile_ci[2], bootstrap_ci[2]), \n                 row.names = c(\"Percentile CI\", \"Bootstrap CI\")), \n      col.names = c(\"2.5%\", \"97.5%\"), digits = 4, \n      caption = \"Parametric Bootstrapping Confidence Intervals\")\n\n\nParametric Bootstrapping Confidence Intervals\n\n\n\n2.5%\n97.5%\n\n\n\n\nPercentile CI\n0.5978\n0.7889\n\n\nBootstrap CI\n0.6079\n0.7989"
  },
  {
    "objectID": "Bootstrap_Prac.html#question-4-relative-risk",
    "href": "Bootstrap_Prac.html#question-4-relative-risk",
    "title": "Bootstrapping Practical",
    "section": "Question 4: Relative Risk",
    "text": "Question 4: Relative Risk\nTable 1 gives rates of cardiovascular disease for subjects with high or low blood pressure. The high-blood pressure group was 2.12 times as likely to develop the disease.\n\nHigh &lt;- \"55/3338 = 0.0165\"\nLow &lt;- \"21/2676 = 0.0078\"\nRR &lt;- \"2.12\"\n\ntab &lt;- data.frame(rbind(High, Low, RR))\nrow.names(tab) &lt;- c(\"high\", \"low\", \"relative risk\")\n\nkable(tab, col.names = c(\"blood pressure\", \"cardiovascular disease\"))\n\n\n\n\nblood pressure\ncardiovascular disease\n\n\n\n\nhigh\n55/3338 = 0.0165\n\n\nlow\n21/2676 = 0.0078\n\n\nrelative risk\n2.12\n\n\n\n\n\nFind the following (your answers should correspond, approximately, to the values in brackets):\n\nbias (0.11)\nbootstrap SE (0.62)\npercentile bootstrap interval: (1.3, 3.7)\nbasic bootstrap interval\n\nHint: Observations are binary. There are two groups.\nAdditionally:\n\nIs there an increased risk of cardiovascular disease with high blood pressure?\nIs the estimate of relative risk biased?\nWhich of the two confidence intervals is better?"
  },
  {
    "objectID": "Parallelisation_Prac.html",
    "href": "Parallelisation_Prac.html",
    "title": "Parallelisation Practical",
    "section": "",
    "text": "Use a foreach loop to repeat 100 times:\n\nGenerate a random sample from an exponential distribution with mean 1\nCalculate mean and variance\nRow-bind your results (rbind) (results = mean and variance).\n\n\nN &lt;- 10000\nn &lt;- 100\n\ngenerated_samples &lt;- foreach(1:N, .combine = rbind) %do% \n  {\n    sample &lt;- rexp(n, rate = 1)\n    c(mean(sample), var(sample))\n  }\n\ncolMeans(generated_samples)\n\n[1] 1.000102 1.002008"
  },
  {
    "objectID": "Parallelisation_Prac.html#question-1",
    "href": "Parallelisation_Prac.html#question-1",
    "title": "Parallelisation Practical",
    "section": "",
    "text": "Use a foreach loop to repeat 100 times:\n\nGenerate a random sample from an exponential distribution with mean 1\nCalculate mean and variance\nRow-bind your results (rbind) (results = mean and variance).\n\n\nN &lt;- 10000\nn &lt;- 100\n\ngenerated_samples &lt;- foreach(1:N, .combine = rbind) %do% \n  {\n    sample &lt;- rexp(n, rate = 1)\n    c(mean(sample), var(sample))\n  }\n\ncolMeans(generated_samples)\n\n[1] 1.000102 1.002008"
  },
  {
    "objectID": "Parallelisation_Prac.html#question-2",
    "href": "Parallelisation_Prac.html#question-2",
    "title": "Parallelisation Practical",
    "section": "Question 2",
    "text": "Question 2\nUse the doParallel package and foreach to bootstrap the median for the galaxies data (in library MASS).\nIf the foreach function needs access to data or a function from a certain package, this can be done by adding the .packages='MASS' (for example) argument.\nHow does processing time compare to that of serial processing? If each iteration’s run time is small, relative to the amount of data that needs to be loaded and returned, parallel processing might not actually speed up the total run time. Bootstrapping is relatively small: draw a sample, calculate a statistic. It might only start making a difference if each chunk becomes large relatively to the overheads of data transfer. Experiment with this. Try doing 1000 bootstrap samples at a time instead of managing single bootstrap samples.\n\ntotal_num_boot &lt;- 10000\nB              &lt;- total_num_boot    # Number of bootstrap Samples\nn              &lt;- length(galaxies)  # Number of samples per\n\ncl &lt;- makeCluster(7)\nregisterDoParallel(cl)\n\n# For small task (1 sample) - 1 thread\nsystem.time(\n{\n  median_boot &lt;- foreach(i = 1:B, .packages = \"MASS\", .combine = rbind) %do% \n  {\n    Sys.sleep(0.001)\n    median(sample(galaxies, n, replace = TRUE))\n  }\n})\n\n   user  system elapsed \n  2.886   0.103  15.605 \n\n# For a small task (1 sample per thread)\nsystem.time(\n{\n  median_boot &lt;- foreach(i = 1:B, .packages = \"MASS\", .combine = rbind) %dopar% \n  {\n    Sys.sleep(0.001)\n    median(sample(galaxies, n, replace = TRUE))\n  }\n})\n\n   user  system elapsed \n  1.354   0.223   2.905 \n\n# For a small task (1000 sample per thread)\nnum_per_boot   &lt;- 1000\nB_multi &lt;- total_num_boot/num_per_boot\n\nsystem.time(\n{\n  median_boot &lt;- foreach(i = 1:B_multi, .packages = \"MASS\", .combine = rbind) %dopar% \n  {\n    Sys.sleep(0.001)\n    replicate(num_per_boot, median(sample(galaxies, n, replace = TRUE)))\n  }\n})\n\n   user  system elapsed \n  0.003   0.000   0.057 \n\nstopCluster(cl)"
  },
  {
    "objectID": "Parallelisation_Prac.html#question-3",
    "href": "Parallelisation_Prac.html#question-3",
    "title": "Parallelisation Practical",
    "section": "Question 3",
    "text": "Question 3\nEstimate coverage of a percentile bootstrap confidence interval for the following scenario: sample of size 50 from an exponential distribution with mean 1.\n\nboot_exp_ci &lt;- function(samples, alpha)\n{\n  # Bootstrapping Means and Finding Quantile\n  \n  sample_mean  &lt;- replicate(1000, mean(sample(samples, replace = TRUE)))\n  cis &lt;- quantile(sample_mean, c(alpha, 1 - alpha))\n  \n  return(cis)\n}\n\nn    &lt;- 50\nrate &lt;- 1\nB    &lt;- 1000\n\ncl &lt;- makeCluster(7)\nregisterDoParallel(cl)\n\ngenerated_cis &lt;- foreach(1:B, .combine = rbind) %dopar%\n{\n  samples &lt;- rexp(n, rate)\n  boot_exp_ci(samples, 0.025)\n}\n\nstopCluster(cl)\n\nprop_cov &lt;- sum(1/rate &gt;= generated_cis[,1] & 1/rate &lt;= generated_cis[,2]) / B"
  },
  {
    "objectID": "Parallelisation_Prac.html#question-4",
    "href": "Parallelisation_Prac.html#question-4",
    "title": "Parallelisation Practical",
    "section": "Question 4",
    "text": "Question 4\nThe package iterators provides several functions that can be used to create sequences for the foreach function. For example, the irnorm function creates an object that iterates over vectors of normally distributed random numbers. It is useful when you need to use random variables drawn from one distribution in an expression that is run in parallel.\nIn this exercise, use the foreach and irnorm functions to iterate over 3 vectors, each containing 5 random variables. Find the largest value in each vector, and print those largest values.\nBefore running the foreach function set the seed to 1234.\n\ncl &lt;- makeCluster(3)\nregisterDoParallel(cl)\n\nset.seed(1234)\nnorms &lt;- irnorm(n = 5, mean = 0, sd = 1)\n\nforeach(i = 1:3, .combine = rbind, .packages = 'iterators') %dopar%\n{\n  max(nextElem(norms))\n}\n\n               [,1]\nresult.1 -0.1336491\nresult.2  0.5786814\nresult.3  1.1681747\n\nstopCluster(cl)\n\n\ntask_func &lt;- function(iter)\n{\n  return(max(iterators::nextElem(norms)))\n}\n\n# For parLapply function\ncl &lt;- makeCluster(getOption(\"cl.cores\", 4))\n\nsystem.time(\n{\n  set.seed(1234)\n  norms &lt;- irnorm(n = 5, mean = 0, sd = 1)\n  clusterExport(cl, \"norms\")\n  parLapply(cl, norms, task_func)\n})\n\n   user  system elapsed \n  0.001   0.000   0.003 \n\n?clusterExport\nstopCluster(cl)\n\n# For foreach function\ncl &lt;- makeCluster(4)\nregisterDoParallel(cl)\n\nsystem.time(\n{\n  set.seed(1234)\n  norms &lt;- irnorm(n = 5, mean = 0, sd = 1)\n  \n  foreach(i = 1:3, .combine = rbind, .packages = 'iterators') %dopar%\n  {\n    task_func(norms)\n  }\n})\n\n   user  system elapsed \n  0.002   0.000   0.020 \n\nstopCluster(cl)\n\n# For replicate function\nsystem.time(\n{\n  set.seed(1234)\n  norms &lt;- irnorm(n = 5, mean = 0, sd = 1)\n  output &lt;- replicate(3, task_func(norms))\n})\n\n   user  system elapsed \n  0.001   0.000   0.001"
  }
]